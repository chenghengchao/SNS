# -*- coding: utf-8 -*-
import urllib2
import urllib
from bs4 import BeautifulSoup
import MySQLdb
import os

# baseUrl = 'http://s.visitbeijing.com.cn/index.php'
user_agent = 'User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36 SE 2.X MetaSr 1.0'
headers = {'User-Agent': user_agent}
values = {
    'm': 'content',
    'c': 'search',
    'catid': 7,
    'theme': 0,
    'area': 0,
    'crowd': 0,
    'level': 0,
    'ticselect': 0,
    'page': 1
}
def getContent(page):
    '''获取网页信息'''
    values = {
    'm': 'content',
    'c': 'search',
    'catid': 7,
    'theme': 0,
    'area': 0,
    'crowd': 0,
    'level': 0,
    'ticselect': 0,
    'page': page
}


    data = urllib.urlencode(values)
    # baseUrl.replace('@PAGE', '1')
    # print baseUrl
    # print data

    # url= 'http://s.visitbeijing.com.cn/index.php?m=content&c=search&catid=7&theme=0&area=0&crowd=0&level=0&ticselect=0&page='
    # url = url + str(page)

    try:
        f=urllib.urlopen("http://s.visitbeijing.com.cn/index.php?%s" % data)
        # response = urllib2.urlopen(url)
        # req = urllib2.Request(url, headers)
        # response = urllib2.urlopen(req)
        # the_page = response.read()
        the_page = f.read()
        # print the_page
        return the_page
    except urllib2.HTTPError,e:
        print e.code
        print e.reason
        # print e.read()
    # save2file(the_page, 'page.txt')
    #


def parseContent(content):
    '''解析网页'''
    soup = BeautifulSoup(content, "lxml")
    scenceList = soup.select('div.list')
    # save2file(scenceList)
    # print scenceList
    for i in scenceList:
        href = i.select('a')
        # print href
        url = href[0]['href'] #景点链接
        print url
        img = i.select('img')
        imgurl = img[0]['src']
        print imgurl

        # os._exit(0)

        conn = conn = MySQLdb.connect(host='202.112.113.203', user='sxw', passwd='0845', port=3306, charset='utf8')
        cur = conn.cursor()
        conn.select_db('sns')
        cur.execute("update sns.scenes_list set imgurl ='"+imgurl+"' where url ='"+url+"'")
        conn.commit()
        cur.close()
        conn.close()


        # if cur.execute("select url from scenes_list where url='"+url+"'") > 0:
        #     print "here"
        #     continue
        
        # save2db(name, url, grade, price, guide, address, spe)


def spider():
    '''循环爬取'''
    for i in range(107):
        if i == 0:
            continue
        content = getContent(i)
        if content:
            parseContent(content)



def save2db(name, url, grade, price, guide, address, spe):
    '''保存到数据库'''
    conn = MySQLdb.connect(host='202.112.113.203', user='sxw', passwd='0845', port=3306, charset='utf8')
    cur = conn.cursor()
    conn.select_db('sns')
    # cur.execute("insert into sns.distance(fromid, toid, distance) values('%d', '%d', '%f')" % (tmplat + 1, tmplng + 1, float(distance)))
    cur.execute("insert into sns.scenes_list(name, url, grade, price, guide, address, special) VALUES ('%s', '%s', '%s', '%s', '%s', '%s', '%s')" %(name, url, grade, price, guide, address, spe))
    conn.commit()
    cur.close()
    conn.close()


def save2file(filename, content):
    '''没什么用'''
    f = open(filename, 'wb+')
    f.write(content)
    f.close()


if __name__ == '__main__':
    '''主函数'''
    # content = getContent(1)
    # parseContent(content)
    spider()
